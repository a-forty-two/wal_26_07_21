{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wal_02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMj+WX1gNWJuwjopIF/Tsay",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a-forty-two/wal_26_07_21/blob/main/wal_02_deepervsthickermodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAKCvIWYbHrh",
        "outputId": "241be34c-5ad3-4686-fce8-75aa9772544b"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VQ1SLushZFU"
      },
      "source": [
        "from tensorflow import keras\n",
        "# code/bug-> Natural Language Processing\n",
        "# not my binary language=> no ready made dictionary \n",
        "imdb = keras.datasets.imdb"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Fgw60PviEon",
        "outputId": "3514da7f-7098-425d-ea7c-d3887b0698ba"
      },
      "source": [
        "(xtrain,ytrain),(xtest,ytest) = imdb.load_data(num_words=10000)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpVJjX_MpsJr",
        "outputId": "8eebebda-437d-421c-d1e6-1ce2b26af921"
      },
      "source": [
        "xtrain.shape\n",
        "xtest.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0og7dPxiRLa",
        "outputId": "c49f2319-2aec-44d6-b06e-ed3b010ae57a"
      },
      "source": [
        "dir(imdb)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " '_sys',\n",
              " 'get_word_index',\n",
              " 'load_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBSIZRJKiU4y",
        "outputId": "880065fe-9363-40d0-a563-de1eb8fe6772"
      },
      "source": [
        "print(xtrain[0])\n",
        "# 2X2=2-> training\n",
        "# 2X3=6 => testing\n",
        "\n",
        "# training data-> x = y, 2X2=4, 2X50=100=> y=mx+c\n",
        "# test-> x, y=? -> y= m*xtest + c\n",
        "# 2X4 = 42, no-> 8. Error = 8-42 = -36\n",
        "# 2X4 = 9, no-> 8, Error = 8-9 = -1 \n",
        "\n",
        "# OVERFITTING-> \n",
        "\n",
        "\n",
        "# All data-> training data, testing data \n",
        "# some x,y went to train the model\n",
        "# rest of x,y went to verify the model "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0aYxuHXifWJ",
        "outputId": "a97f04ae-d21b-429d-ab8c-d6992845d281"
      },
      "source": [
        "print(ytrain[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdAKbgybim3l"
      },
      "source": [
        "dictionary = imdb.get_word_index()\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtoc1VOnlR84",
        "outputId": "f1b26c85-f47f-4df2-f7db-0cbc0bb03752"
      },
      "source": [
        "\n",
        "dictionary['hello']"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4822"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2iKJ-61lW8d"
      },
      "source": [
        "word_index = {word:(encoding+3) for word,encoding in dictionary.items()}\n",
        "word_index['<PAD>'] = 0\n",
        "word_index['<START>'] = 1\n",
        "word_index['<UNK>'] = 2  # unknown words\n",
        "word_index['<UNUSED>'] = 3\n",
        "worddictionary = { encoding:word  for word,encoding in word_index.items()   }"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "eCuvP-COpBjR",
        "outputId": "1434fafa-29c9-4399-8528-c9f51d89c2c3"
      },
      "source": [
        "worddictionary[100]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'could'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ngTowvhpE1-"
      },
      "source": [
        "\n",
        "\n",
        "def decoder(sampleinput):\n",
        "  return \" \".join([worddictionary[word] for word in sampleinput])\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "nc799fxjpV_E",
        "outputId": "62bf6067-b57f-4ab2-ec1a-c28201202411"
      },
      "source": [
        "decoder(xtrain[100])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"<START> i am a great fan of david lynch and have everything that he's made on dvd except for hotel room the 2 hour twin peaks movie so when i found out about this i immediately grabbed it and and what is this it's a bunch of <UNK> drawn black and white cartoons that are loud and foul mouthed and unfunny maybe i don't know what's good but maybe this is just a bunch of crap that was <UNK> on the public under the name of david lynch to make a few bucks too let me make it clear that i didn't care about the foul language part but had to keep <UNK> the sound because my neighbors might have all in all this is a highly disappointing release and may well have just been left in the <UNK> box set as a curiosity i highly recommend you don't spend your money on this 2 out of 10\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lixPJp0apZUj",
        "outputId": "2bc678f9-6af8-4ada-aa96-7d04923ca14e"
      },
      "source": [
        "ytrain[100]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHuPw0eXp9Ls",
        "outputId": "68bd249d-b31f-4749-b707-95916805b82f"
      },
      "source": [
        "xtrain[0:5]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
              "       list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 8255, 2, 349, 2637, 148, 605, 2, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
              "       list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
              "       list([1, 4, 2, 2, 33, 2804, 4, 2040, 432, 111, 153, 103, 4, 1494, 13, 70, 131, 67, 11, 61, 2, 744, 35, 3715, 761, 61, 5766, 452, 9214, 4, 985, 7, 2, 59, 166, 4, 105, 216, 1239, 41, 1797, 9, 15, 7, 35, 744, 2413, 31, 8, 4, 687, 23, 4, 2, 7339, 6, 3693, 42, 38, 39, 121, 59, 456, 10, 10, 7, 265, 12, 575, 111, 153, 159, 59, 16, 1447, 21, 25, 586, 482, 39, 4, 96, 59, 716, 12, 4, 172, 65, 9, 579, 11, 6004, 4, 1615, 5, 2, 7, 5168, 17, 13, 7064, 12, 19, 6, 464, 31, 314, 11, 2, 6, 719, 605, 11, 8, 202, 27, 310, 4, 3772, 3501, 8, 2722, 58, 10, 10, 537, 2116, 180, 40, 14, 413, 173, 7, 263, 112, 37, 152, 377, 4, 537, 263, 846, 579, 178, 54, 75, 71, 476, 36, 413, 263, 2504, 182, 5, 17, 75, 2306, 922, 36, 279, 131, 2895, 17, 2867, 42, 17, 35, 921, 2, 192, 5, 1219, 3890, 19, 2, 217, 4122, 1710, 537, 2, 1236, 5, 736, 10, 10, 61, 403, 9, 2, 40, 61, 4494, 5, 27, 4494, 159, 90, 263, 2311, 4319, 309, 8, 178, 5, 82, 4319, 4, 65, 15, 9225, 145, 143, 5122, 12, 7039, 537, 746, 537, 537, 15, 7979, 4, 2, 594, 7, 5168, 94, 9096, 3987, 2, 11, 2, 4, 538, 7, 1795, 246, 2, 9, 2, 11, 635, 14, 9, 51, 408, 12, 94, 318, 1382, 12, 47, 6, 2683, 936, 5, 6307, 2, 19, 49, 7, 4, 1885, 2, 1118, 25, 80, 126, 842, 10, 10, 2, 2, 4726, 27, 4494, 11, 1550, 3633, 159, 27, 341, 29, 2733, 19, 4185, 173, 7, 90, 2, 8, 30, 11, 4, 1784, 86, 1117, 8, 3261, 46, 11, 2, 21, 29, 9, 2841, 23, 4, 1010, 2, 793, 6, 2, 1386, 1830, 10, 10, 246, 50, 9, 6, 2750, 1944, 746, 90, 29, 2, 8, 124, 4, 882, 4, 882, 496, 27, 2, 2213, 537, 121, 127, 1219, 130, 5, 29, 494, 8, 124, 4, 882, 496, 4, 341, 7, 27, 846, 10, 10, 29, 9, 1906, 8, 97, 6, 236, 2, 1311, 8, 4, 2, 7, 31, 7, 2, 91, 2, 3987, 70, 4, 882, 30, 579, 42, 9, 12, 32, 11, 537, 10, 10, 11, 14, 65, 44, 537, 75, 2, 1775, 3353, 2, 1846, 4, 2, 7, 154, 5, 4, 518, 53, 2, 2, 7, 3211, 882, 11, 399, 38, 75, 257, 3807, 19, 2, 17, 29, 456, 4, 65, 7, 27, 205, 113, 10, 10, 2, 4, 2, 2, 9, 242, 4, 91, 1202, 2, 5, 2070, 307, 22, 7, 5168, 126, 93, 40, 2, 13, 188, 1076, 3222, 19, 4, 2, 7, 2348, 537, 23, 53, 537, 21, 82, 40, 2, 13, 2, 14, 280, 13, 219, 4, 2, 431, 758, 859, 4, 953, 1052, 2, 7, 5991, 5, 94, 40, 25, 238, 60, 2, 4, 2, 804, 2, 7, 4, 9941, 132, 8, 67, 6, 22, 15, 9, 283, 8, 5168, 14, 31, 9, 242, 955, 48, 25, 279, 2, 23, 12, 1685, 195, 25, 238, 60, 796, 2, 4, 671, 7, 2804, 5, 4, 559, 154, 888, 7, 726, 50, 26, 49, 7008, 15, 566, 30, 579, 21, 64, 2574]),\n",
              "       list([1, 249, 1323, 7, 61, 113, 10, 10, 13, 1637, 14, 20, 56, 33, 2401, 18, 457, 88, 13, 2626, 1400, 45, 3171, 13, 70, 79, 49, 706, 919, 13, 16, 355, 340, 355, 1696, 96, 143, 4, 22, 32, 289, 7, 61, 369, 71, 2359, 5, 13, 16, 131, 2073, 249, 114, 249, 229, 249, 20, 13, 28, 126, 110, 13, 473, 8, 569, 61, 419, 56, 429, 6, 1513, 18, 35, 534, 95, 474, 570, 5, 25, 124, 138, 88, 12, 421, 1543, 52, 725, 6397, 61, 419, 11, 13, 1571, 15, 1543, 20, 11, 4, 2, 5, 296, 12, 3524, 5, 15, 421, 128, 74, 233, 334, 207, 126, 224, 12, 562, 298, 2167, 1272, 7, 2601, 5, 516, 988, 43, 8, 79, 120, 15, 595, 13, 784, 25, 3171, 18, 165, 170, 143, 19, 14, 5, 7224, 6, 226, 251, 7, 61, 113])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkvbGkB5qVbD"
      },
      "source": [
        "# sales-> country, us, warehouse-> \n",
        "# 0-> 100\n",
        "# 1-> 200\n",
        "# 3-> 150\n",
        "\n",
        "# 0-> 120-> padded with 20 zeros\n",
        "# 1-> 120-> last 80 numbers chopped\n",
        "# 2-> 120-> last 30 numbers chopped\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnHZX_KZrqa4"
      },
      "source": [
        "# read your input\n",
        "\n",
        "# i am a good boy = 1\n",
        "# i am a bad boy = 0\n",
        "# we are not so good = 0\n",
        "\n",
        "# i*w1 + am*w2 + a*w3 + good *w4 + boy *w5 + bad * 0 + we*0 +b1 = 1\n",
        "# i*w1 + am*w2 + a*w3 + good *0 + boy *w5 + bad * w6 +b2= 0"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ai4MGD90xS2",
        "outputId": "94a9fdd0-b88f-46ef-e802-ec8de5bbf6f9"
      },
      "source": [
        "xtrain_padded = keras.preprocessing.sequence.pad_sequences(xtrain, value=0, padding='post',\n",
        "                                                           truncating='post', maxlen=256)\n",
        "xtest_padded = keras.preprocessing.sequence.pad_sequences(xtest, value=0, padding='post', \n",
        "                                                          truncating='post', maxlen=256)\n",
        "print(decoder(xtrain[642]))\n",
        "print(decoder(xtrain_padded[642])) # LONG SENTENCE CHOPPED\n",
        "print(decoder(xtrain[262])) # SHORT SENTENCE PADDED\n",
        "print(decoder(xtrain_padded[262]))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<START> if deborah messing were not already cast as grace this might be a tolerable film however it is simply another story of a frustrated <UNK> with issues who hires a paid <UNK> <UNK> <UNK> she reads about in a time magazine article to travel to london for her sister's london wedding how new is this plot br br neither funny nor remotely romantic the wedding date <UNK> over the storyline of <UNK> sex by bride and best man and paid for <UNK> to pass off the film as four <UNK> without hugh and definitely a dead end deal for the naive <UNK> who is ignorant to the sexual history of his bride amy adams while messing has <UNK> the repressed princess 30 something woman with a failed relationship history her neurotic and drunken moves on yet another faux beau is simply the <UNK> of her tv series if this woman is an actress get a role that does not rehash what is already on prime time br br lots of drunken female bonding <UNK> visual jokes and <UNK> in a <UNK> nice but the film is a bore with the obvious happy ending expected messing sequel divorce date\n",
            "<START> if deborah messing were not already cast as grace this might be a tolerable film however it is simply another story of a frustrated <UNK> with issues who hires a paid <UNK> <UNK> <UNK> she reads about in a time magazine article to travel to london for her sister's london wedding how new is this plot br br neither funny nor remotely romantic the wedding date <UNK> over the storyline of <UNK> sex by bride and best man and paid for <UNK> to pass off the film as four <UNK> without hugh and definitely a dead end deal for the naive <UNK> who is ignorant to the sexual history of his bride amy adams while messing has <UNK> the repressed princess 30 something woman with a failed relationship history her neurotic and drunken moves on yet another faux beau is simply the <UNK> of her tv series if this woman is an actress get a role that does not rehash what is already on prime time br br lots of drunken female bonding <UNK> visual jokes and <UNK> in a <UNK> nice but the film is a bore with the obvious happy ending expected messing sequel divorce date <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "<START> i've just finished viewing the 1st disc in a 4 disc 26 episodes collection created in <UNK> with the <UNK> film television archive <UNK> entertainment inc so far aside from the 1st episode the image quality is quite good the dvd box is shown on the title page here on imdb br br mr <UNK> is just as charming as when i first saw it 5 years old at the time and <UNK> cox is truly endearing in this role if you're in the mood for quiet comedy that <UNK> up on you as opposed to hitting you over the head you'll treasure this chance to experience all the wonderful characters you might remember from your childhood although some of the gags are a bit corny most are ingenious and well executed and even the corny ones are fun this is one tv series that lives up to my early childhood memories of it\n",
            "<START> i've just finished viewing the 1st disc in a 4 disc 26 episodes collection created in <UNK> with the <UNK> film television archive <UNK> entertainment inc so far aside from the 1st episode the image quality is quite good the dvd box is shown on the title page here on imdb br br mr <UNK> is just as charming as when i first saw it 5 years old at the time and <UNK> cox is truly endearing in this role if you're in the mood for quiet comedy that <UNK> up on you as opposed to hitting you over the head you'll treasure this chance to experience all the wonderful characters you might remember from your childhood although some of the gags are a bit corny most are ingenious and well executed and even the corny ones are fun this is one tv series that lives up to my early childhood memories of it <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdD83GBD1TfX"
      },
      "source": [
        "HP_dictionary_size = 10000\n",
        "HP_m1_l1= 16 \n",
        "HP_m2_l1= 32\n",
        "HP_m3_l1= 128\n",
        "# each word broken into 16 vectors! total= 10,000 X 16 weights for vectors\n",
        "HP_m1_l2= 64\n",
        "HP_m2_l2= 128\n",
        "HP_m3_l2= 256\n",
        "\n",
        "HP_epoch = 50 #timelines-> how many times do you want to train the model?\n",
        "# NEURAL NETWORKS-> exposed to your data for 1 row/1 batch at time\n",
        "# Forward Propagation-> values of weights and bias is calculated \n",
        "# 1 epoch is over-> and i get value of y\n",
        "# this y will contain error\n",
        "# Backword Propagation-> dy/dx of entire network-> results in new values of w,b\n",
        "# MOre the no. of epochs-> more are the FORWARD+BACKWORD Propagations\n",
        "# More is the Integrate (make it big)-> differentiate (break it down)\n",
        "\n",
        "\n",
        "HP_maxlen = 256\n",
        "HP_batch_size = 128   "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4kvlpj68lQs"
      },
      "source": [
        "layer1m3 = keras.layers.Embedding(10000,  16)\n",
        "layer2m3 = keras.layers.GlobalAveragePooling1D()\n",
        "# weights -> (-1,1)\n",
        "# words-> positive\n",
        "# equation-> words*weights = pos*neg | pos*pos\n",
        "layer3m3 = keras.layers.Dense(HP_m2_l2, activation='relu')\n",
        "layer3bm3 = keras.layers.Dense(1024, activation='relu')\n",
        "layer4m3 = keras.layers.Dense(2, activation='softmax')\n",
        "\n",
        "alllayersm3 = [layer1m3, layer2m3, layer3m3,layer3bm3, layer4m3]\n",
        "# SEQUENTIAL DEEP LEARNING MODEL \n",
        "model4 = keras.models.Sequential(alllayersm3)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mbk3Yco3-rrI"
      },
      "source": [
        "layer1m2 = keras.layers.Embedding(10000,  HP_m2_l1)\n",
        "layer2m2 = keras.layers.GlobalAveragePooling1D()\n",
        "# weights -> (-1,1)\n",
        "# words-> positive\n",
        "# equation-> words*weights = pos*neg | pos*pos\n",
        "layer3m2 = keras.layers.Dense(HP_m2_l2, activation='relu')\n",
        "layer4m2 = keras.layers.Dense(2, activation='softmax')\n",
        "alllayersm2 = [layer1m2, layer2m2, layer3m2, layer4m2]\n",
        "# SEQUENTIAL DEEP LEARNING MODEL \n",
        "model2 = keras.models.Sequential(alllayersm2)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFoZleJRYw4J"
      },
      "source": [
        "model4.compile(loss=tf.losses.sparse_categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCxHv-_MzbUh"
      },
      "source": [
        "model2.compile(loss=tf.losses.sparse_categorical_crossentropy, optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7LAj3h8vejm"
      },
      "source": [
        ""
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydUWM-irZWly",
        "outputId": "ca37b9d7-8b7a-4d85-9e76-bbc4255c1a25"
      },
      "source": [
        "model4.summary()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, None, 16)          160000    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_4 ( (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 128)               2176      \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 1024)              132096    \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 2)                 2050      \n",
            "=================================================================\n",
            "Total params: 296,322\n",
            "Trainable params: 296,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kf0cNjvTasZg",
        "outputId": "3fa66493-2748-47c4-fb25-a388054743fd"
      },
      "source": [
        "model2.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 32)          320000    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_1 ( (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               4224      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 324,482\n",
            "Trainable params: 324,482\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YU9KmMwfNpa"
      },
      "source": [
        ""
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfdMPFEXazJK"
      },
      "source": [
        "xval = xtest_padded[:10000]\n",
        "xtest_padded_reduced = xtest_padded[10000:]\n",
        "yval = ytest[:10000]\n",
        "ytest_reduced = ytest[10000:]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c65Gkuz8faX7",
        "outputId": "279ce67f-a14d-4cb7-d948-16ebc123e17c"
      },
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "history_m1 = model1.fit(xtrain_padded, ytrain, epochs=50, batch_size=HP_batch_size, validation_data = (xval, yval), verbose=0) \n",
        "end_time = time.time()\n",
        "print('Time Taken = ' + str(end_time-start_time))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time Taken = 82.53606247901917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gYk6tEjzi1_",
        "outputId": "0cbb8e6f-e073-4941-aa01-25d56c7880e8"
      },
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "history_m4 = model4.fit(xtrain_padded, ytrain, epochs=50, batch_size=HP_batch_size, validation_data = (xval, yval), verbose=0) \n",
        "end_time = time.time()\n",
        "print('Time Taken = ' + str(end_time-start_time))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time Taken = 120.54905486106873\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0cfx0rEcYtq",
        "outputId": "08b79d11-6003-4f94-95d5-1f6454b369ef"
      },
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "history_m2 = model2.fit(xtrain_padded, ytrain, epochs=50, batch_size=HP_batch_size, validation_data = (xval, yval), verbose=0) \n",
        "end_time = time.time()\n",
        "print('Time Taken = ' + str(end_time-start_time))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time Taken = 108.58299779891968\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivvwVeF-c-xW"
      },
      "source": [
        "predictions1 = model1.predict(xtest_padded_reduced)\n",
        "predictions2 = model2.predict(xtest_padded_reduced)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5epqveLi0Ezb"
      },
      "source": [
        "predictions4 =  model4.predict(xtest_padded_reduced)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCZfUu8qeTER",
        "outputId": "06ddb616-2d12-463e-db03-358aad136546"
      },
      "source": [
        "predictions1[2200]\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9.9999964e-01, 3.4025965e-07], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgTMqvhU0JvW",
        "outputId": "9d5c7f76-4753-4239-d082-2bda5341be6a"
      },
      "source": [
        "predictions4[2200]"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9.9999952e-01, 4.4061466e-07], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GA3EOpsayHp6",
        "outputId": "d662c13c-54f1-4adb-dce0-ef29517c8551"
      },
      "source": [
        "predictions2[2200]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9.9999964e-01, 3.9803146e-07], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5bqqSAQucY9",
        "outputId": "a225aa81-07f9-43da-e484-89ea677a2b6e"
      },
      "source": [
        "ytest_reduced[2200]"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "GqEJz0HlyFVx",
        "outputId": "c0788780-a5e6-418f-966d-121a1d6b92a0"
      },
      "source": [
        "decoder(xtest_padded_reduced[2200])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"<START> hey there army sgt i'm sorry dude but being a sgt in the army and being in the army national guard does not make you <UNK> to comment on a marine movie you are not a marine and just because you wear a uniform doesn't mean you can relate to being a marine we simply are the best we have the hardest training yes we have big heads about ourselves but hey when you are the best you like to <UNK> your stuff i was in the iraq invasion and in i fought next to soldiers you are not <UNK> to say anything about my marine <UNK> i hate to be the one that starts the whole which branch is better but you have no right to say you are <UNK> to judge a marine movie oh yeah we are <UNK> <UNK> not <UNK> that's the biggest clue you have no idea about what you are talking about yeah we do not curse at <UNK> anymore tell me how is <UNK> at someone going to make them a better marine how will me hitting someone make a marine a better marine yes it is a <UNK> boot camp from what i went through but we are dealing with different times and people we are training people who are over all smarter than our generations <UNK> we want smarter <UNK> not <UNK> and anyone who signs up to be a marine in the first place has a dedication to be the best his country has to\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j42vy7swyE4l",
        "outputId": "22a355a1-aa03-495f-84d8-51b0361e64d7"
      },
      "source": [
        "perf1 = model1.evaluate(xtest_padded, ytest)\n",
        "perf1"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 1s 2ms/step - loss: 2.6386 - accuracy: 0.8102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.638556957244873, 0.8102399706840515]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voanrpLV0Ve6",
        "outputId": "4a9120e9-930f-460f-bf41-f9e8b99c5d6a"
      },
      "source": [
        "perf4 = model4.evaluate(xtest_padded, ytest)\n",
        "perf4"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 2s 2ms/step - loss: 2.6050 - accuracy: 0.8248\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.6050143241882324, 0.8248000144958496]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEPN0d9cy62s",
        "outputId": "0ba581a1-a629-4fc9-df50-101b7c158758"
      },
      "source": [
        "perf2 = model2.evaluate(xtest_padded, ytest)\n",
        "perf2"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 2s 2ms/step - loss: 2.1499 - accuracy: 0.8172\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.1499383449554443, 0.8172399997711182]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRKkFSzpbuYO",
        "outputId": "4c92919a-dee7-4dbc-9a06-fedd7b0a9519"
      },
      "source": [
        "history_m1 = model1.fit(xtrain_padded, ytrain, epochs=HP_epoch, batch_size=32, validation_data = (xval, yval), verbose=1) "
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0094 - accuracy: 0.9974 - val_loss: 2.3808 - val_accuracy: 0.8089\n",
            "Epoch 2/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0669 - accuracy: 0.9778 - val_loss: 1.6632 - val_accuracy: 0.8185\n",
            "Epoch 3/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0174 - accuracy: 0.9936 - val_loss: 1.7231 - val_accuracy: 0.8153\n",
            "Epoch 4/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0093 - accuracy: 0.9971 - val_loss: 1.8984 - val_accuracy: 0.8153\n",
            "Epoch 5/50\n",
            "782/782 [==============================] - 4s 6ms/step - loss: 0.0057 - accuracy: 0.9982 - val_loss: 1.9398 - val_accuracy: 0.8144\n",
            "Epoch 6/50\n",
            "782/782 [==============================] - 4s 6ms/step - loss: 0.0101 - accuracy: 0.9964 - val_loss: 2.0570 - val_accuracy: 0.8123\n",
            "Epoch 7/50\n",
            "782/782 [==============================] - 4s 6ms/step - loss: 0.0234 - accuracy: 0.9909 - val_loss: 2.1150 - val_accuracy: 0.8167\n",
            "Epoch 8/50\n",
            "782/782 [==============================] - 4s 6ms/step - loss: 0.0210 - accuracy: 0.9919 - val_loss: 1.9429 - val_accuracy: 0.8109\n",
            "Epoch 9/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0098 - accuracy: 0.9965 - val_loss: 2.0040 - val_accuracy: 0.8167\n",
            "Epoch 10/50\n",
            "782/782 [==============================] - 4s 6ms/step - loss: 0.0060 - accuracy: 0.9979 - val_loss: 2.1263 - val_accuracy: 0.8127\n",
            "Epoch 11/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0108 - accuracy: 0.9956 - val_loss: 2.0602 - val_accuracy: 0.8128\n",
            "Epoch 12/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0118 - accuracy: 0.9961 - val_loss: 2.1061 - val_accuracy: 0.8121\n",
            "Epoch 13/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0133 - accuracy: 0.9952 - val_loss: 2.1845 - val_accuracy: 0.8145\n",
            "Epoch 14/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0108 - accuracy: 0.9961 - val_loss: 2.2151 - val_accuracy: 0.8104\n",
            "Epoch 15/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0088 - accuracy: 0.9969 - val_loss: 2.2463 - val_accuracy: 0.8125\n",
            "Epoch 16/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0157 - accuracy: 0.9945 - val_loss: 2.1794 - val_accuracy: 0.8111\n",
            "Epoch 17/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0102 - accuracy: 0.9966 - val_loss: 2.1984 - val_accuracy: 0.8106\n",
            "Epoch 18/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0105 - accuracy: 0.9964 - val_loss: 2.2548 - val_accuracy: 0.8094\n",
            "Epoch 19/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0063 - accuracy: 0.9978 - val_loss: 2.3063 - val_accuracy: 0.8128\n",
            "Epoch 20/50\n",
            "782/782 [==============================] - 4s 6ms/step - loss: 0.0114 - accuracy: 0.9961 - val_loss: 2.3367 - val_accuracy: 0.8096\n",
            "Epoch 21/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0082 - accuracy: 0.9972 - val_loss: 2.3136 - val_accuracy: 0.8108\n",
            "Epoch 22/50\n",
            "782/782 [==============================] - 4s 6ms/step - loss: 0.0114 - accuracy: 0.9960 - val_loss: 2.3363 - val_accuracy: 0.8103\n",
            "Epoch 23/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0065 - accuracy: 0.9978 - val_loss: 2.3103 - val_accuracy: 0.8097\n",
            "Epoch 24/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0042 - accuracy: 0.9989 - val_loss: 2.4464 - val_accuracy: 0.8107\n",
            "Epoch 25/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0092 - accuracy: 0.9972 - val_loss: 2.3662 - val_accuracy: 0.8115\n",
            "Epoch 26/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0132 - accuracy: 0.9954 - val_loss: 2.3231 - val_accuracy: 0.8114\n",
            "Epoch 27/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0096 - accuracy: 0.9966 - val_loss: 2.3324 - val_accuracy: 0.8110\n",
            "Epoch 28/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0046 - accuracy: 0.9986 - val_loss: 2.4095 - val_accuracy: 0.8101\n",
            "Epoch 29/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0052 - accuracy: 0.9981 - val_loss: 2.4871 - val_accuracy: 0.8091\n",
            "Epoch 30/50\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.0117 - accuracy: 0.9956 - val_loss: 2.4723 - val_accuracy: 0.8099\n",
            "Epoch 31/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0085 - accuracy: 0.9974 - val_loss: 2.4773 - val_accuracy: 0.8097\n",
            "Epoch 32/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0104 - accuracy: 0.9967 - val_loss: 2.4515 - val_accuracy: 0.8100\n",
            "Epoch 33/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 2.5546 - val_accuracy: 0.8091\n",
            "Epoch 34/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0061 - accuracy: 0.9977 - val_loss: 2.6117 - val_accuracy: 0.8094\n",
            "Epoch 35/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0105 - accuracy: 0.9963 - val_loss: 2.5300 - val_accuracy: 0.8066\n",
            "Epoch 36/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0046 - accuracy: 0.9984 - val_loss: 2.5808 - val_accuracy: 0.8066\n",
            "Epoch 37/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0061 - accuracy: 0.9979 - val_loss: 2.5280 - val_accuracy: 0.8077\n",
            "Epoch 38/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0070 - accuracy: 0.9972 - val_loss: 2.6508 - val_accuracy: 0.8101\n",
            "Epoch 39/50\n",
            "782/782 [==============================] - 4s 6ms/step - loss: 0.0088 - accuracy: 0.9970 - val_loss: 2.6870 - val_accuracy: 0.8090\n",
            "Epoch 40/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0103 - accuracy: 0.9966 - val_loss: 2.5290 - val_accuracy: 0.8077\n",
            "Epoch 41/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0041 - accuracy: 0.9987 - val_loss: 2.6295 - val_accuracy: 0.8101\n",
            "Epoch 42/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 2.6426 - val_accuracy: 0.8075\n",
            "Epoch 43/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0067 - accuracy: 0.9976 - val_loss: 2.7937 - val_accuracy: 0.8102\n",
            "Epoch 44/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0094 - accuracy: 0.9970 - val_loss: 2.6649 - val_accuracy: 0.8116\n",
            "Epoch 45/50\n",
            "782/782 [==============================] - 4s 6ms/step - loss: 0.0040 - accuracy: 0.9987 - val_loss: 2.6563 - val_accuracy: 0.8044\n",
            "Epoch 46/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0027 - accuracy: 0.9990 - val_loss: 2.7267 - val_accuracy: 0.8085\n",
            "Epoch 47/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0069 - accuracy: 0.9977 - val_loss: 2.8003 - val_accuracy: 0.8072\n",
            "Epoch 48/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0069 - accuracy: 0.9974 - val_loss: 2.7031 - val_accuracy: 0.8092\n",
            "Epoch 49/50\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0085 - accuracy: 0.9970 - val_loss: 2.7203 - val_accuracy: 0.8063\n",
            "Epoch 50/50\n",
            "782/782 [==============================] - 4s 6ms/step - loss: 0.0070 - accuracy: 0.9971 - val_loss: 2.6812 - val_accuracy: 0.8073\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HLBqJhe-ral"
      },
      "source": [
        "# ML=y = mx+c\n",
        "# DL=y' = w1*f(y) + w2*f1(y) + w3.....w161088*f161087(y) + b1...b65\n",
        "# y' = [0,1]\n",
        "# \n",
        "# weights = input_size*output_size\n",
        "# biases = output_size \n",
        "# HOW will the loss be calculated? -> LOSS FUNCTION\n",
        "# Once it is calculated, how to reduce it?-> OPTIMIZER FUNCTION\n",
        "# Preprocessed Inputs -> not all data is equal\n",
        "# before we had columns-> highly structured\n",
        "# unstructured data-> different lengths-> MATRIX \n",
        "# EMBEDDED-> MATRIX-> structured data\n",
        "# GLobalAveragePooling-> to transform Matrix to 1-D tensor /number series\n",
        "# Deep Learning-> 1-D tensors\n",
        "# Any deep learning equation\n",
        "# build a network to read raw data and process into 1-D tensors\n",
        "# 2 Dense layers to calculate w,b [pattern of data]-> last 2 layers\n",
        "# From input to output-> HIDDEN LAYERS-> Golbal Average Pooling , 1 Dense Layer\n"
      ],
      "execution_count": 32,
      "outputs": []
    }
  ]
}