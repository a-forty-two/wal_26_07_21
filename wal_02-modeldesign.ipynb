{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wal_02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNpYpDaEGnQoMUeJCqumBAW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a-forty-two/wal_26_07_21/blob/main/wal_02-modeldesign.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAKCvIWYbHrh",
        "outputId": "ed50a5d5-0659-4f70-d6ef-cc47f105c869"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VQ1SLushZFU"
      },
      "source": [
        "from tensorflow import keras\n",
        "# code/bug-> Natural Language Processing\n",
        "# not my binary language=> no ready made dictionary \n",
        "imdb = keras.datasets.imdb"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Fgw60PviEon",
        "outputId": "bfb16214-5663-4540-b15d-882c71b0d1c6"
      },
      "source": [
        "(xtrain,ytrain),(xtest,ytest) = imdb.load_data()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpVJjX_MpsJr",
        "outputId": "fc452305-0265-43a8-b06b-48ad8d660568"
      },
      "source": [
        "xtrain.shape\n",
        "xtest.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0og7dPxiRLa",
        "outputId": "6e08415f-37e9-4490-f18a-b197c1825800"
      },
      "source": [
        "dir(imdb)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " '_sys',\n",
              " 'get_word_index',\n",
              " 'load_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBSIZRJKiU4y",
        "outputId": "6e678cc7-8e51-4c1c-9390-864c4253daf6"
      },
      "source": [
        "print(xtrain[0])\n",
        "# 2X2=2-> training\n",
        "# 2X3=6 => testing\n",
        "\n",
        "# training data-> x = y, 2X2=4, 2X50=100=> y=mx+c\n",
        "# test-> x, y=? -> y= m*xtest + c\n",
        "# 2X4 = 42, no-> 8. Error = 8-42 = -36\n",
        "# 2X4 = 9, no-> 8, Error = 8-9 = -1 \n",
        "\n",
        "# OVERFITTING-> \n",
        "\n",
        "\n",
        "# All data-> training data, testing data \n",
        "# some x,y went to train the model\n",
        "# rest of x,y went to verify the model "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0aYxuHXifWJ",
        "outputId": "f4e7a061-e800-4e74-f7e3-71ad062c995b"
      },
      "source": [
        "print(ytrain[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdAKbgybim3l",
        "outputId": "baf7b907-8b4d-4703-8055-40b84fe27f87"
      },
      "source": [
        "dictionary = imdb.get_word_index()\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtoc1VOnlR84",
        "outputId": "09c4eb39-db71-4039-f552-d401fd7dda29"
      },
      "source": [
        "\n",
        "dictionary['hello']"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4822"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2iKJ-61lW8d"
      },
      "source": [
        "word_index = {word:(encoding+3) for word,encoding in dictionary.items()}\n",
        "word_index['<PAD>'] = 0\n",
        "word_index['<START>'] = 1\n",
        "word_index['<UNK>'] = 2  # unknown words\n",
        "word_index['<UNUSED>'] = 3\n",
        "worddictionary = { encoding:word  for word,encoding in word_index.items()   }"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "eCuvP-COpBjR",
        "outputId": "463b0cf2-a165-4e19-ad0f-6d9c97c4d61f"
      },
      "source": [
        "worddictionary[100]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'could'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ngTowvhpE1-"
      },
      "source": [
        "\n",
        "\n",
        "def decoder(sampleinput):\n",
        "  return \" \".join([worddictionary[word] for word in sampleinput])\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "nc799fxjpV_E",
        "outputId": "abb63b13-53fd-4a62-b261-dd16e91cf5bf"
      },
      "source": [
        "decoder(xtrain[100])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"<START> i am a great fan of david lynch and have everything that he's made on dvd except for hotel room the 2 hour twin peaks movie so when i found out about this i immediately grabbed it and and what is this it's a bunch of crudely drawn black and white cartoons that are loud and foul mouthed and unfunny maybe i don't know what's good but maybe this is just a bunch of crap that was foisted on the public under the name of david lynch to make a few bucks too let me make it clear that i didn't care about the foul language part but had to keep adjusting the sound because my neighbors might have all in all this is a highly disappointing release and may well have just been left in the deluxe box set as a curiosity i highly recommend you don't spend your money on this 2 out of 10\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lixPJp0apZUj",
        "outputId": "99fb613d-3764-4411-b461-34054de20c1d"
      },
      "source": [
        "ytrain[100]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHuPw0eXp9Ls",
        "outputId": "2be6e6e2-5d3c-4bb0-fa7d-e75b223cf9cd"
      },
      "source": [
        "xtrain[0:5]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
              "       list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
              "       list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
              "       list([1, 4, 18609, 16085, 33, 2804, 4, 2040, 432, 111, 153, 103, 4, 1494, 13, 70, 131, 67, 11, 61, 15305, 744, 35, 3715, 761, 61, 5766, 452, 9214, 4, 985, 7, 64317, 59, 166, 4, 105, 216, 1239, 41, 1797, 9, 15, 7, 35, 744, 2413, 31, 8, 4, 687, 23, 4, 33929, 7339, 6, 3693, 42, 38, 39, 121, 59, 456, 10, 10, 7, 265, 12, 575, 111, 153, 159, 59, 16, 1447, 21, 25, 586, 482, 39, 4, 96, 59, 716, 12, 4, 172, 65, 9, 579, 11, 6004, 4, 1615, 5, 23005, 7, 5168, 17, 13, 7064, 12, 19, 6, 464, 31, 314, 11, 87564, 6, 719, 605, 11, 8, 202, 27, 310, 4, 3772, 3501, 8, 2722, 58, 10, 10, 537, 2116, 180, 40, 14, 413, 173, 7, 263, 112, 37, 152, 377, 4, 537, 263, 846, 579, 178, 54, 75, 71, 476, 36, 413, 263, 2504, 182, 5, 17, 75, 2306, 922, 36, 279, 131, 2895, 17, 2867, 42, 17, 35, 921, 18435, 192, 5, 1219, 3890, 19, 20523, 217, 4122, 1710, 537, 20341, 1236, 5, 736, 10, 10, 61, 403, 9, 47289, 40, 61, 4494, 5, 27, 4494, 159, 90, 263, 2311, 4319, 309, 8, 178, 5, 82, 4319, 4, 65, 15, 9225, 145, 143, 5122, 12, 7039, 537, 746, 537, 537, 15, 7979, 4, 18665, 594, 7, 5168, 94, 9096, 3987, 15242, 11, 28280, 4, 538, 7, 1795, 246, 56615, 9, 10161, 11, 635, 14, 9, 51, 408, 12, 94, 318, 1382, 12, 47, 6, 2683, 936, 5, 6307, 10197, 19, 49, 7, 4, 1885, 13699, 1118, 25, 80, 126, 842, 10, 10, 47289, 18223, 4726, 27, 4494, 11, 1550, 3633, 159, 27, 341, 29, 2733, 19, 4185, 173, 7, 90, 16376, 8, 30, 11, 4, 1784, 86, 1117, 8, 3261, 46, 11, 25837, 21, 29, 9, 2841, 23, 4, 1010, 26747, 793, 6, 13699, 1386, 1830, 10, 10, 246, 50, 9, 6, 2750, 1944, 746, 90, 29, 16376, 8, 124, 4, 882, 4, 882, 496, 27, 33029, 2213, 537, 121, 127, 1219, 130, 5, 29, 494, 8, 124, 4, 882, 496, 4, 341, 7, 27, 846, 10, 10, 29, 9, 1906, 8, 97, 6, 236, 11120, 1311, 8, 4, 23643, 7, 31, 7, 29851, 91, 22793, 3987, 70, 4, 882, 30, 579, 42, 9, 12, 32, 11, 537, 10, 10, 11, 14, 65, 44, 537, 75, 11876, 1775, 3353, 12716, 1846, 4, 11286, 7, 154, 5, 4, 518, 53, 13243, 11286, 7, 3211, 882, 11, 399, 38, 75, 257, 3807, 19, 18223, 17, 29, 456, 4, 65, 7, 27, 205, 113, 10, 10, 33058, 4, 22793, 10359, 9, 242, 4, 91, 1202, 11377, 5, 2070, 307, 22, 7, 5168, 126, 93, 40, 18223, 13, 188, 1076, 3222, 19, 4, 13465, 7, 2348, 537, 23, 53, 537, 21, 82, 40, 18223, 13, 33195, 14, 280, 13, 219, 4, 52788, 431, 758, 859, 4, 953, 1052, 12283, 7, 5991, 5, 94, 40, 25, 238, 60, 35410, 4, 15812, 804, 27767, 7, 4, 9941, 132, 8, 67, 6, 22, 15, 9, 283, 8, 5168, 14, 31, 9, 242, 955, 48, 25, 279, 22148, 23, 12, 1685, 195, 25, 238, 60, 796, 13713, 4, 671, 7, 2804, 5, 4, 559, 154, 888, 7, 726, 50, 26, 49, 7008, 15, 566, 30, 579, 21, 64, 2574]),\n",
              "       list([1, 249, 1323, 7, 61, 113, 10, 10, 13, 1637, 14, 20, 56, 33, 2401, 18, 457, 88, 13, 2626, 1400, 45, 3171, 13, 70, 79, 49, 706, 919, 13, 16, 355, 340, 355, 1696, 96, 143, 4, 22, 32, 289, 7, 61, 369, 71, 2359, 5, 13, 16, 131, 2073, 249, 114, 249, 229, 249, 20, 13, 28, 126, 110, 13, 473, 8, 569, 61, 419, 56, 429, 6, 1513, 18, 35, 534, 95, 474, 570, 5, 25, 124, 138, 88, 12, 421, 1543, 52, 725, 6397, 61, 419, 11, 13, 1571, 15, 1543, 20, 11, 4, 22016, 5, 296, 12, 3524, 5, 15, 421, 128, 74, 233, 334, 207, 126, 224, 12, 562, 298, 2167, 1272, 7, 2601, 5, 516, 988, 43, 8, 79, 120, 15, 595, 13, 784, 25, 3171, 18, 165, 170, 143, 19, 14, 5, 7224, 6, 226, 251, 7, 61, 113])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkvbGkB5qVbD"
      },
      "source": [
        "# sales-> country, us, warehouse-> \n",
        "# 0-> 100\n",
        "# 1-> 200\n",
        "# 3-> 150\n",
        "\n",
        "# 0-> 120-> padded with 20 zeros\n",
        "# 1-> 120-> last 80 numbers chopped\n",
        "# 2-> 120-> last 30 numbers chopped\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnHZX_KZrqa4"
      },
      "source": [
        "# read your input\n",
        "\n",
        "# i am a good boy = 1\n",
        "# i am a bad boy = 0\n",
        "# we are not so good = 0\n",
        "\n",
        "# i*w1 + am*w2 + a*w3 + good *w4 + boy *w5 + bad * 0 + we*0 +b1 = 1\n",
        "# i*w1 + am*w2 + a*w3 + good *0 + boy *w5 + bad * w6 +b2= 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ai4MGD90xS2",
        "outputId": "16654336-432a-488a-f6fd-b42042e3e5c9"
      },
      "source": [
        "xtrain_padded = keras.preprocessing.sequence.pad_sequences(xtrain, value=0, padding='post',\n",
        "                                                           truncating='post', maxlen=256)\n",
        "xtest_padded = keras.preprocessing.sequence.pad_sequences(xtest, value=0, padding='post', \n",
        "                                                          truncating='post', maxlen=256)\n",
        "print(decoder(xtrain[642]))\n",
        "print(decoder(xtrain_padded[642])) # LONG SENTENCE CHOPPED\n",
        "print(decoder(xtrain[262])) # SHORT SENTENCE PADDED\n",
        "print(decoder(xtrain_padded[262]))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<START> if deborah messing were not already cast as grace this might be a tolerable film however it is simply another story of a frustrated spinster with issues who hires a paid escort dermot mulroney she reads about in a time magazine article to travel to london for her sister's london wedding how new is this plot br br neither funny nor remotely romantic the wedding date slides over the storyline of deceptive sex by bride and best man and paid for escorts to pass off the film as four weddings without hugh and definitely a dead end deal for the naive groom who is ignorant to the sexual history of his bride amy adams while messing has perfected the repressed princess 30 something woman with a failed relationship history her neurotic and drunken moves on yet another faux beau is simply the restating of her tv series if this woman is an actress get a role that does not rehash what is already on prime time br br lots of drunken female bonding cricket visual jokes and mulroney in a towel nice but the film is a bore with the obvious happy ending expected messing sequel divorce date\n",
            "<START> if deborah messing were not already cast as grace this might be a tolerable film however it is simply another story of a frustrated spinster with issues who hires a paid escort dermot mulroney she reads about in a time magazine article to travel to london for her sister's london wedding how new is this plot br br neither funny nor remotely romantic the wedding date slides over the storyline of deceptive sex by bride and best man and paid for escorts to pass off the film as four weddings without hugh and definitely a dead end deal for the naive groom who is ignorant to the sexual history of his bride amy adams while messing has perfected the repressed princess 30 something woman with a failed relationship history her neurotic and drunken moves on yet another faux beau is simply the restating of her tv series if this woman is an actress get a role that does not rehash what is already on prime time br br lots of drunken female bonding cricket visual jokes and mulroney in a towel nice but the film is a bore with the obvious happy ending expected messing sequel divorce date <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "<START> i've just finished viewing the 1st disc in a 4 disc 26 episodes collection created in conjunction with the ucla film television archive s'more entertainment inc so far aside from the 1st episode the image quality is quite good the dvd box is shown on the title page here on imdb br br mr peepers is just as charming as when i first saw it 5 years old at the time and wally cox is truly endearing in this role if you're in the mood for quiet comedy that sneaks up on you as opposed to hitting you over the head you'll treasure this chance to experience all the wonderful characters you might remember from your childhood although some of the gags are a bit corny most are ingenious and well executed and even the corny ones are fun this is one tv series that lives up to my early childhood memories of it\n",
            "<START> i've just finished viewing the 1st disc in a 4 disc 26 episodes collection created in conjunction with the ucla film television archive s'more entertainment inc so far aside from the 1st episode the image quality is quite good the dvd box is shown on the title page here on imdb br br mr peepers is just as charming as when i first saw it 5 years old at the time and wally cox is truly endearing in this role if you're in the mood for quiet comedy that sneaks up on you as opposed to hitting you over the head you'll treasure this chance to experience all the wonderful characters you might remember from your childhood although some of the gags are a bit corny most are ingenious and well executed and even the corny ones are fun this is one tv series that lives up to my early childhood memories of it <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdD83GBD1TfX"
      },
      "source": [
        "HP_dictionary_size = 10000\n",
        "HP_m1_l1= 16 \n",
        "HP_m2_l1= 32\n",
        "HP_m3_l1= 128\n",
        "# each word broken into 16 vectors! total= 10,000 X 16 weights for vectors\n",
        "HP_m1_l2= 64\n",
        "HP_m2_l2= 128\n",
        "HP_m3_l2= 256\n",
        "\n",
        "HP_epoch = 50 #timelines-> how many times do you want to train the model?\n",
        "# NEURAL NETWORKS-> exposed to your data for 1 row/1 batch at time\n",
        "# Forward Propagation-> values of weights and bias is calculated \n",
        "# 1 epoch is over-> and i get value of y\n",
        "# this y will contain error\n",
        "# Backword Propagation-> dy/dx of entire network-> results in new values of w,b\n",
        "# MOre the no. of epochs-> more are the FORWARD+BACKWORD Propagations\n",
        "# More is the Integrate (make it big)-> differentiate (break it down)\n",
        "\n",
        "\n",
        "HP_maxlen = 256\n",
        "HP_batch_size = 128   "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4kvlpj68lQs"
      },
      "source": [
        "layer1m1 = keras.layers.Embedding(10000,  HP_m1_l1)\n",
        "layer2m1 = keras.layers.GlobalAveragePooling1D()\n",
        "# weights -> (-1,1)\n",
        "# words-> positive\n",
        "# equation-> words*weights = pos*neg | pos*pos\n",
        "layer3m1 = keras.layers.Dense(HP_m1_l2, activation='relu')\n",
        "layer4m1 = keras.layers.Dense(1, activation='sigmoid')\n",
        "alllayersm1 = [layer1m1, layer2m1, layer3m1, layer4m1]\n",
        "# SEQUENTIAL DEEP LEARNING MODEL \n",
        "model1 = keras.models.Sequential(alllayersm1)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mbk3Yco3-rrI"
      },
      "source": [
        "layer1m2 = keras.layers.Embedding(10000,  HP_m2_l1)\n",
        "layer2m2 = keras.layers.GlobalAveragePooling1D()\n",
        "# weights -> (-1,1)\n",
        "# words-> positive\n",
        "# equation-> words*weights = pos*neg | pos*pos\n",
        "layer3m2 = keras.layers.Dense(HP_m2_l2, activation='relu')\n",
        "layer4m2 = keras.layers.Dense(1, activation='sigmoid')\n",
        "alllayersm2 = [layer1m2, layer2m2, layer3m2, layer4m2]\n",
        "# SEQUENTIAL DEEP LEARNING MODEL \n",
        "model2 = keras.models.Sequential(alllayersm2)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HLBqJhe-ral"
      },
      "source": [
        "# HOW will the loss be calculated? -> LOSS FUNCTION\n",
        "# Once it is calculated, how to reduce it?-> OPTIMIZER FUNCTION\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}